{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ts8y9oaVtFWf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-vision\n",
        "!pip install pdf2image\n",
        "!apt-get install -y poppler-utils"
      ],
      "metadata": {
        "collapsed": true,
        "id": "99jQ5T5nF0He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import csv\n",
        "from pdf2image import convert_from_path\n",
        "from google.cloud import vision\n",
        "\n",
        "#configuración de Google Cloud Vision\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/drive/MyDrive/Dataset/google-ocr.json\"\n",
        "client = vision.ImageAnnotatorClient()\n",
        "\n",
        "# directorios de entrada (pdfs) y salida (OCR)\n",
        "BASE_DIR = \"/content/drive/MyDrive/Dataset/JW_Org_Quechua_Spanish\"\n",
        "folders = {\n",
        "    \"esp\": os.path.join(BASE_DIR, \"esp\"),\n",
        "    \"quz\": os.path.join(BASE_DIR, \"quz\")\n",
        "}\n",
        "output_folder = os.path.join(BASE_DIR, \"OCR_Texts\")  # carpeta de salida\n",
        "os.makedirs(output_folder, exist_ok=True)  # crear dicha carpeta si no existe\n",
        "\n",
        "#función para extraer texto de una imagen con api de Google OCR\n",
        "def extract_text_google_vision(image_path):\n",
        "    \"\"\"Extrae texto de una imagen usando Google Cloud Vision API\"\"\"\n",
        "    with io.open(image_path, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    image = vision.Image(content=content)\n",
        "    response = client.text_detection(image=image)\n",
        "    texts = response.text_annotations\n",
        "\n",
        "    if texts:\n",
        "        return texts[0].description.strip()  # Eliminación de espacios extra\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "# procesar pdfs en ambas carpetas\n",
        "for lang, folder in folders.items():\n",
        "    lang_output_folder = os.path.join(output_folder, lang)  # carpeta de salida por idioma\n",
        "    os.makedirs(lang_output_folder, exist_ok=True)  # crear si no existe (por idioma)\n",
        "\n",
        "    for file in sorted(os.listdir(folder)):\n",
        "        if not file.endswith(\".pdf\"):\n",
        "            continue\n",
        "\n",
        "        pdf_path = os.path.join(folder, file)\n",
        "        output_tsv_path = os.path.join(lang_output_folder, file.replace(\".pdf\", \".tsv\"))\n",
        "\n",
        "        # evitar reprocesar si ya existe\n",
        "        if os.path.exists(output_tsv_path):\n",
        "            print(f\"Ya procesado: {output_tsv_path}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Procesando: {pdf_path} ...\")\n",
        "\n",
        "        # convertir PDF a imágenes para aplicar el OCR\n",
        "        images = convert_from_path(pdf_path, dpi=300)\n",
        "\n",
        "        #extraer texto de cada página\n",
        "        extracted_data = []\n",
        "        for i, image in enumerate(images):\n",
        "            image_path = f\"temp_page_{i+1}.jpg\"\n",
        "            image.save(image_path, \"JPEG\")  # guardar temporalmente en el espacio de trabajo de colab\n",
        "\n",
        "            text = extract_text_google_vision(image_path)\n",
        "            if text:\n",
        "                extracted_data.append([i + 1, text])  # guardar número de página y texto correspondiente\n",
        "\n",
        "            os.remove(image_path)  #eliminar imagen temporal\n",
        "\n",
        "        #guardar en TSV solo si hay texto extraído\n",
        "        if extracted_data:\n",
        "            with open(output_tsv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "                writer = csv.writer(f, delimiter=\"\\t\")\n",
        "                writer.writerow([\"pagina\", \"texto\"])  #cabecera del tsv\n",
        "                writer.writerows(extracted_data)\n",
        "\n",
        "            print(f\"Guardado: {output_tsv_path}\")\n",
        "        else:\n",
        "            print(f\"No se detectó texto en {pdf_path}.\")"
      ],
      "metadata": {
        "id": "gxlm7fXTIGsh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "#función para limpiar texto:\n",
        "#saltos de línea, normalizar espacios, eliminar símbolos no deseados (no alfabéticos)\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'[^\\w\\s.,;:!?¿¡\"()\\'\\-áéíóúñÁÉÍÓÚÑ]', '', text)\n",
        "    return text\n",
        "\n",
        "#definir rutas base para guardar versión limpia\n",
        "base_input = \"/content/drive/MyDrive/Dataset/JW_Org_Quechua_Spanish/OCR_Texts\"\n",
        "base_output = \"/content/drive/MyDrive/Dataset/JW_Org_Quechua_Spanish/OCR_Cleaned_Texts\"\n",
        "\n",
        "for lang in [\"esp\", \"quz\"]:\n",
        "    input_folder = os.path.join(base_input, lang)\n",
        "    output_folder = os.path.join(base_output, lang)\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    for filename in sorted(os.listdir(input_folder)):\n",
        "        if not filename.endswith(\".tsv\"):\n",
        "            continue\n",
        "\n",
        "        file_path = os.path.join(input_folder, filename)\n",
        "        output_path = os.path.join(output_folder, filename)\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", encoding=\"utf-8\")\n",
        "            if \"texto\" not in df.columns:\n",
        "                print(f\"No se encontró columna 'texto' en: {filename}\")\n",
        "                continue\n",
        "\n",
        "            df[\"texto\"] = df[\"texto\"].apply(clean_text)\n",
        "            df.to_csv(output_path, sep=\"\\t\", index=False, encoding=\"utf-8\")\n",
        "            print(f\"Archivo limpio guardado: {output_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error procesando {filename}: {e}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nMDJ3aMkrqY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "#correcciones ortográficas para el quechua collao (glotalizado o aspirado) obtenidas de Manual de escritura\n",
        "correcciones = {\n",
        "    r'\\bnuqa\\b|\\bnoqa\\b': 'ñuqa',\n",
        "    r'\\bnuqawan\\b|\\bnoqawan\\b': 'ñuqawan',\n",
        "    r'\\bqulqi\\b|\\bqolqe\\b|\\bqollqe\\b': 'qullqi',\n",
        "    r'\\bsalqa\\b|\\bsalq’a\\b': 'sallqa',\n",
        "    r'\\btanpa\\b|\\bt\\'anpa\\b': \"t'ampa\",\n",
        "    r'\\bmachkay\\b|\\bmashkay\\b': 'maskhay',\n",
        "    r'\\bjawapi\\b|\\bqawapi\\b': 'hawapi',\n",
        "    r'\\balqu\\b|\\balqo\\b|\\ballku\\b': 'allqu',\n",
        "    r'\\bluqu\\b|\\bruqhu\\b': 'ruqu',\n",
        "    r'\\bllihlla\\b|\\blliqlla\\b': 'lliklla',\n",
        "    r'\\bqatun\\b|\\bjatun\\b|\\batun\\b': 'hatun',\n",
        "    r'\\bjampi\\b|\\bqampi\\b': 'hampi',\n",
        "    r'\\bwashka\\b|\\bwachka\\b': 'waskha',\n",
        "    r'\\bkuchka\\b|\\bkushka\\b': 'kuska',\n",
        "    r'\\bmijuna\\b|\\bmihuna\\b': 'mikhuna',\n",
        "    r'\\blaphi\\b|\\brap’i\\b|\\blapi\\b': 'raphi',\n",
        "    r'\\bhayay\\b|\\bjayay\\b': 'qayay',\n",
        "    r'\\bwaq’ay\\b|\\bwahay\\b|\\bwajay\\b': 'waqay',\n",
        "    r'\\bkilli\\b|\\bkhilli\\b': 'qhilli',\n",
        "    r'\\bqhillu\\b|\\bk\\'illu\\b|\\bkillu\\b': \"q'illu\",\n",
        "    r'\\ballim\\b': 'allin',\n",
        "    r'\\bwasimpi\\b': 'wasinpi',\n",
        "    r'\\bpijchu\\b|\\bpihchu\\b|\\bpiqchu\\b': 'pikchu',\n",
        "    r'\\bhuaita\\b': 'wayta',\n",
        "    r'\\bmaitu\\b': \"mayt'u\",\n",
        "    r'\\bhuauqei\\b': 'wawqiy',\n",
        "    r'\\bccatimuhuay\\b': 'qatimuway',\n",
        "    r'\\bhuaina\\b': 'wayna',\n",
        "    r'\\byahuar\\b': 'yawar',\n",
        "    r'\\bpishqa\\b|\\bpisqa\\b|\\bphisqa\\b': 'pichqa',\n",
        "    r'\\bashka\\b|\\baskha\\b': 'achka',\n",
        "    r'\\bch\\'uqay\\b|\\bchhuqay\\b': 'chuqay',\n",
        "    r'\\bwisq\\'ay\\b|\\bwishqay\\b': \"wichq'ay\",\n",
        "    r'\\bhuq\\b|\\bhuj\\b': 'huk',\n",
        "    r'\\bkhayra\\b|\\bq\\'ayra\\b': \"k'ayra\",\n",
        "    r'\\bchiqaq\\b': 'chiqap',\n",
        "    r'\\bllamphu\\b': \"llamp'u\",\n",
        "    r'\\bhaykaq\\b|\\bhayk\\'aq\\b': \"hayk'ap\",\n",
        "    r'\\bhata\\b|\\bkhata\\b': 'qhata',\n",
        "    r'\\busqaylla\\b|\\buthqaylla\\b': 'utqaylla',\n",
        "    r'\\bqasqi\\b': 'qatqi',\n",
        "    r'\\bt\\'iqtiy\\b|\\btiqthiy\\b|\\btiqt\\'iy\\b': 'tiqtiy',\n",
        "    r'\\bllant\\'u\\b': \"llanthu\",\n",
        "}\n",
        "\n",
        "def corregir_texto(texto, log):\n",
        "    palabras = texto.split()\n",
        "    corregidas = []\n",
        "\n",
        "    for palabra in palabras:\n",
        "        original = palabra\n",
        "        for patron, reemplazo in correcciones.items():\n",
        "            if re.fullmatch(patron, palabra):\n",
        "                log.append((original, reemplazo))\n",
        "                palabra = reemplazo\n",
        "                break\n",
        "        corregidas.append(palabra)\n",
        "\n",
        "    return ' '.join(corregidas)\n",
        "\n",
        "#rutas de export\n",
        "input_path = \"/content/drive/MyDrive/Dataset/JW_Org_Quechua_Spanish/OCR_Cleaned_Texts/quz\"\n",
        "output_path = \"/content/drive/MyDrive/Dataset/JW_Org_Quechua_Spanish/OCR_Standardized_Texts/quz\"\n",
        "log_path = \"/content/drive/MyDrive/Dataset/JW_Org_Quechua_Spanish/OCR_Standardized_Texts/quz_logs\"\n",
        "\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "os.makedirs(log_path, exist_ok=True)\n",
        "\n",
        "#procesamiento de archivos\n",
        "for archivo in sorted(os.listdir(input_path)):\n",
        "    if not archivo.endswith(\".tsv\"):\n",
        "        continue\n",
        "\n",
        "    input_file = os.path.join(input_path, archivo)\n",
        "    output_file = os.path.join(output_path, archivo)\n",
        "    log_file = os.path.join(log_path, archivo.replace(\".tsv\", \"_log.txt\"))\n",
        "\n",
        "    df = pd.read_csv(input_file, sep=\"\\t\", encoding=\"utf-8\")\n",
        "    if \"texto\" not in df.columns:\n",
        "        print(f\"No se encontró columna 'texto' en {archivo}\")\n",
        "        continue\n",
        "\n",
        "    cambios = []\n",
        "    df[\"texto\"] = df[\"texto\"].apply(lambda t: corregir_texto(t, cambios))\n",
        "    df.to_csv(output_file, sep=\"\\t\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for original, corregido in cambios:\n",
        "            f.write(f\"{original} → {corregido}\\n\")\n",
        "\n",
        "    print(f\"{archivo} corregido. Cambios: {len(cambios)}\")"
      ],
      "metadata": {
        "id": "BiRWQTgeL0V_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "\n",
        "#patrón de puntuación seguida de mayúscula\n",
        "def split_oraciones(texto):\n",
        "    return re.split(r'(?<=[.?!])\\s+(?=[A-ZÁÉÍÓÚÑ])', texto.strip())\n",
        "\n",
        "def calcular_metadata(tsv_path):\n",
        "    try:\n",
        "        df = pd.read_csv(tsv_path, sep=\"\\t\", encoding=\"utf-8\")\n",
        "        texto_completo = \" \".join(df[\"texto\"].dropna().tolist())\n",
        "        oraciones = split_oraciones(texto_completo)\n",
        "        palabras = texto_completo.split()\n",
        "        palabras_unicas = set(palabras)\n",
        "\n",
        "        return {\n",
        "            \"Documento\": os.path.basename(tsv_path),\n",
        "            \"N° Oraciones\": len(oraciones),\n",
        "            \"Palabras Totales\": len(palabras),\n",
        "            \"Palabras Únicas\": len(palabras_unicas)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando {tsv_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "#carpeta de tsv (el input)\n",
        "base_input = \"/content/drive/MyDrive/Dataset/JW_Org_Quechua_Spanish/OCR_Cleaned_Texts\"\n",
        "\n",
        "resultados = []\n",
        "\n",
        "for lang in [\"esp\", \"quz\"]:\n",
        "    input_folder = os.path.join(base_input, lang)\n",
        "    for filename in sorted(os.listdir(input_folder)):\n",
        "        if not filename.endswith(\".tsv\"):\n",
        "            continue\n",
        "        path = os.path.join(input_folder, filename)\n",
        "        metadata = calcular_metadata(path)\n",
        "        if metadata:\n",
        "            metadata[\"Idioma\"] = lang\n",
        "            resultados.append(metadata)\n",
        "\n",
        "#guardar a csv\n",
        "df_resultado = pd.DataFrame(resultados)\n",
        "output_csv = \"/content/drive/MyDrive/Dataset/JW_Org_Quechua_Spanish/outputs/metadata_r1.csv\"\n",
        "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
        "df_resultado.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
        "print(f\"Metadata guardada en: {output_csv}\")"
      ],
      "metadata": {
        "id": "_h3KAU5O0X3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#leer la metadata ya guardada\n",
        "metadata_path = \"/content/drive/MyDrive/Dataset/JW_Org_Quechua_Spanish/outputs/metadata_r1.csv\"\n",
        "df = pd.read_csv(metadata_path)\n",
        "#Resumen general\n",
        "resumen = {\n",
        "    \"Documentos procesados\": len(df),\n",
        "    \"Idiomas\": df[\"Idioma\"].unique().tolist(),\n",
        "    \"Oraciones (promedio)\": round(df[\"N° Oraciones\"].mean(), 2),\n",
        "    \"Oraciones (mínimo)\": df[\"N° Oraciones\"].min(),\n",
        "    \"Oraciones (máximo)\": df[\"N° Oraciones\"].max(),\n",
        "    \"Palabras Totales (promedio)\": round(df[\"Palabras Totales\"].mean(), 2),\n",
        "    \"Palabras Totales (mínimo)\": df[\"Palabras Totales\"].min(),\n",
        "    \"Palabras Totales (máximo)\": df[\"Palabras Totales\"].max(),\n",
        "    \"Palabras Únicas (promedio)\": round(df[\"Palabras Únicas\"].mean(), 2),\n",
        "    \"Palabras Únicas (mínimo)\": df[\"Palabras Únicas\"].min(),\n",
        "    \"Palabras Únicas (máximo)\": df[\"Palabras Únicas\"].max()\n",
        "}\n",
        "pd.DataFrame([resumen])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PAFJwFMm6qvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Resumen específico por idioma\n",
        "resumen_por_idioma = df.groupby(\"Idioma\").agg({\n",
        "    \"Documento\": \"count\",\n",
        "    \"N° Oraciones\": [\"mean\", \"min\", \"max\"],\n",
        "    \"Palabras Totales\": [\"mean\", \"min\", \"max\"],\n",
        "    \"Palabras Únicas\": [\"mean\", \"min\", \"max\"]\n",
        "}).reset_index()\n",
        "\n",
        "resumen_por_idioma.columns = [\n",
        "    \"Idioma\", \"Documentos\",\n",
        "    \"Oraciones (prom)\", \"Oraciones (min)\", \"Oraciones (max)\",\n",
        "    \"Palabras Totales (prom)\", \"Palabras Totales (min)\", \"Palabras Totales (max)\",\n",
        "    \"Palabras Únicas (prom)\", \"Palabras Únicas (min)\", \"Palabras Únicas (max)\"\n",
        "]\n",
        "resumen_por_idioma = resumen_por_idioma.round(2)\n",
        "resumen_por_idioma"
      ],
      "metadata": {
        "id": "LT6i1gQ27dmV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}